import nest_asyncio
import uvicorn
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
import logging
import asyncio
import torch

# 현재 실행 중인 이벤트 루프에 코루틴을 추가할 수 있도록 패치
nest_asyncio.apply()

# FastAPI 애플리케이션 초기화
app = FastAPI()

# CORS 설정
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 로깅 설정
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# LLM 및 프롬프트 템플릿 초기화
llm = ChatOllama(model="llama-ko-bllossom-server:latest", batch_size=8)

# 프롬프트 템플릿 정의
prompts = {
    "keyPoints": {
        "ko": ChatPromptTemplate.from_messages(
            [
                ("system", "당신은 논문 분석 및 요약 전문가입니다. 제공하는 문장은 논문의 초록입니다. 당신의 임무는 제공된 논문 서론을 분석하고 핵심을 추출하는 것입니다."),
                ("user", "다음 텍스트를 분석하여 추론하고, 핵심을 파악하여 간략한 요약본으로 제공하십시오. 답변은 한국어로 줄바꿈 없이 간략하게 정리해주십시오. 분석할 텍스트::\n{context}"),
            ]
        ),
        "en": ChatPromptTemplate.from_messages(
            [
                ("system", "You are an expert in thesis analysis and summarization. The sentences you provide are abstracts of the thesis. Your mission is to analyze the introduction to the thesis provided and extract the core."),
                ("user", "Analyze and deduce the following text, identify the key and provide it as a brief summary. Text to be analyzed::\n{context}"),
            ]
        ),
    },
    "research_topic": {
        "ko": ChatPromptTemplate.from_messages(
            [
                ("system", "당신은 논문 분석 및 요약 전문가입니다. 제공하는 문장은 논문의 초록입니다. 당신의 임무는 제공된 논문 서론을 분석하고 핵심을 추출하는 것입니다."),
                ("user", "다음 텍스트를 분석하여 연구주제를 파악하십시오. 답변은 제공되는 분석할 텍스트 언어에 따라 한국어로 간략하게 정리해주십시오. 분석할 텍스트::\n{context}"),
            ]
        ),
        "en": ChatPromptTemplate.from_messages(
            [
                ("system", "You are an expert in thesis analysis and summarization. The sentences you provide are abstracts of the thesis. Your mission is to analyze the introduction to the thesis provided and extract the core."),
                ("user", "Identify your research topic by analyzing the following text. Text to be analyzed::\n{context}"),
            ]
        ),
    },
    "research_method": {
        "ko": ChatPromptTemplate.from_messages(
            [
                ("system", "당신은 논문 분석 및 요약 전문가입니다. 제공하는 문장은 논문의 초록입니다. 당신의 임무는 제공된 논문 서론을 분석하고 핵심을 추출하는 것입니다."),
                ("user", "다음 텍스트를 분석하여 연구방법을 파악하십시오. 답변은 한국어로 간략하게 정리해주십시오. 분석할 텍스트::\n{context}"),
            ]
        ),
        "en": ChatPromptTemplate.from_messages(
            [
                ("system", "You are an expert in thesis analysis and summarization. The sentences you provide are abstracts of the thesis. Your mission is to analyze the introduction to the thesis provided and extract the core."),
                ("user", "Identify your research methods by analyzing the following text. Text to be analyzed::\n{context}"),
            ]
        ),
    },
    "research_result": {
        "ko": ChatPromptTemplate.from_messages(
            [
                ("system", "당신은 논문 분석 및 요약 전문가입니다. 제공하는 문장은 논문의 초록입니다. 당신의 임무는 제공된 논문 서론을 분석하고 핵심을 추출하는 것입니다."),
                ("user", "다음 텍스트를 분석하여 연구결과를 파악하십시오. 답변은 한국어로 간략하게 정리해주십시오. 분석할 텍스트::\n{context}"),
            ]
        ),
        "en": ChatPromptTemplate.from_messages(
            [
                ("system", "You are an expert in thesis analysis and summarization. The sentences you provide are abstracts of the thesis. Your mission is to analyze the introduction to the thesis provided and extract the core."),
                ("user", "Please analyze the following text to understand the findings. Text to be analyzed::\n{context}"),
            ]
        ),
    },
}

# 모델 초기화
model, tokenizer = None, None

async def initialize_model():
    global model, tokenizer
    if model is None or tokenizer is None:
        # 모델 로드 및 초기화
        model = ChatOllama(model="llama3:latest", batch_size=8)
        logger.info("Model initialized and loaded successfully")

# FastAPI 시작 시 모델 초기화
@app.on_event("startup")
async def startup_event():
    await initialize_model()

@app.middleware("http")
async def add_charset_to_request(request: Request, call_next):
    request.state.charset = 'utf-8'
    response = await call_next(request)
    return response

@app.post("/summarize/{summary_type}")
async def summarize(summary_type: str, request: Request):
    if summary_type not in prompts:
        raise HTTPException(status_code=400, detail="Invalid summary type")

    data = await request.json()
    language = data.get('language','')
    prompt_template = prompts[summary_type].get(language, prompts[summary_type]['ko'])
    return await generate_summary(request, prompt_template)

async def generate_summary(request: Request, prompt_template: ChatPromptTemplate):
    try:
        data = await request.json()
        context = data.get('text', '')
        if not context:
            logger.warning("No text provided in the request.")
            raise HTTPException(status_code=400, detail="No text provided")
    except Exception as e:
        logger.warning("Invalid JSON in the request.")
        raise HTTPException(status_code=400, detail="Invalid JSON")

    logger.info(f"Received request for summarization: {context}")
    
    try:
        chain = prompt_template | llm
        
        # 동기 함수를 비동기로 실행
        result = await asyncio.get_event_loop().run_in_executor(None, chain.invoke, {"context": context})
        logger.info(f"Generated summary: {result.content}")
    
        summary_lines = result.content.split('\n')
        
        return JSONResponse(content={"summary": summary_lines})
    except Exception as e:
        logger.error("Error during summarization", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == '__main__':
    uvicorn.run(app, host='0.0.0.0', port=5000)
