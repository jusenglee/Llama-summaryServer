from flask import Flask, request, jsonify
from flask_cors import CORS
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, pipeline
from accelerate import init_empty_weights, load_checkpoint_and_dispatch
import os
import torch
import logging
from concurrent.futures import ThreadPoolExecutor

# Flask 애플리케이션 초기화
app = Flask(__name__)
CORS(app)  # 모든 도메인에서의 접근 허용 (차후 

# 로깅 설정
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Hugging Face API 키 설정
os.environ['HUGGINGFACEHUB_API_TOKEN'] = '************'

# 사용 가능한 GPU의 수 확인
num_gpus = torch.cuda.device_count()
logger.info(f"Number of GPUs available: {num_gpus}")

# 각 GPU의 이름 확인
for i in range(num_gpus):
    logger.info(f"GPU {i}: {torch.cuda.get_device_name(i)}")

# 모델 및 토크나이저 로드
model_id = "MLP-KTLim/llama-3-Korean-Bllossom-8B"
local_model_path = "./llama-3-bllossom"

def load_model():
    tokenizer = AutoTokenizer.from_pretrained(local_model_path)
    config = AutoConfig.from_pretrained(local_model_path)
    config.attn_implementation = 'eager'  # attn_implementation 설정 추가
    model = AutoModelForCausalLM.from_pretrained(local_model_path)

    max_memory = {i: "40GB" for i in range(num_gpus)}
    max_memory["cpu"] = "200GB"

    model = load_checkpoint_and_dispatch(
        model,
        checkpoint=local_model_path,
        device_map="balanced",
        offload_folder="offload",
        max_memory=max_memory,
        dtype=torch.float16
    )
    
    return model, tokenizer

# 모델과 토크나이저를 애플리케이션 시작 시 한 번만 로드
model, tokenizer = load_model()

# pipeline 생성
pipeline =  pipeline(
    "text-generation",
    model= model,
    tokenizer=tokenizer,
    model_kwargs={"torch_dtype": torch.bfloat16},
    device_map="auto",
)

pipeline.model.eval()
# 스레드 풀 설정
executor = ThreadPoolExecutor(max_workers=4)

def summarize_document(text, pipe):
    PROMPT = (
        "당신은 논문 분석 및 요약 전문가입니다. 당신의 임무는 제공된 논문 서론을 분석하고 주요 용어, 연구 주제, 연구 방법 및 연구 결과를 추출하는 것입니다. 마지막으로 한국어로 논문서론을 간결하게 요약합니다. "
        "다음 텍스트를 분석하여 주요 용어, 연구 주제, 연구 방법 및 연구 결과를 파악하십시오. "
        "마지막으로 한국어로 논문서론을 간결하게 요약합니다."
    )
    messages = [
    {"role": "system", "content": f"{PROMPT}"},
    {"role": "user", "content": f"{text}"}
    ]

    prompt = pipeline.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
    )
    
    terminators = [
        pipeline.tokenizer.eos_token_id,
        pipeline.tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]
    
    logger.info("Generating summary...")
    logger.info(f"Prompt: {prompt}")
    
    try:
        result = pipeline(
                            prompt,
                            max_new_tokens=2048,
                            eos_token_id=terminators,
                            do_sample=True,
                            temperature=0.6,
                            top_p=0.9
                        )
        logger.info(f"Summary generated successfully. : {result}")
    except Exception as e:
        logger.error(f"Error during text generation: {e}")
        raise

    if not result or len(result) == 0:
        logger.error("No result returned from text generation pipeline.")
        raise ValueError("No result returned from text generation pipeline.")

    logger.info(f"Generated result: {result}")
    
    summary_lines = result[0]['generated_text'][len(prompt):].split('\n')
    logger.info(f"Summary lines: {summary_lines}")

    with app.app_context():
        response = jsonify({"summary": summary_lines})
    return response


@app.route('/summarize', methods=['POST'])
def summarize():
    data = request.get_json()
    if data is None:
        logger.warning("Invalid JSON in the request.")
        return jsonify({"error": "Invalid JSON"}), 400

    context = data.get('text', '')
    if not context:
        logger.warning("No text provided in the request.")
        return jsonify({"error": "No text provided"}), 400
    
    logger.info(f"Received request for summarization: {context}")

    try:
        # 비동기 요청 처리
        future = executor.submit(summarize_document, context, pipeline)
        response = future.result()
        return response
    except Exception as e:
        logger.error("Error during summarization", exc_info=True)
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
